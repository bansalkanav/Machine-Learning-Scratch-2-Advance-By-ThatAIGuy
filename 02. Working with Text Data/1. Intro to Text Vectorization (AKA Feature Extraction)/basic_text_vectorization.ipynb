{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iD4dZM5cSfZN"
   },
   "source": [
    "# **Working with Text Data - Text Transformation (AKA Vectorization)**\n",
    "**AKA Feature Extraction | Engineering | Transformation (Text to Numerical Vector)**\n",
    "\n",
    "### **Text Data**\n",
    "\n",
    "Text Analysis is a major application field for machine learning algorithms. Some of the major application areas of NLP are:\n",
    "1. Spell Checker, Keyword Search, etc\n",
    "2. Sentiment Analysis, Spam Classification\n",
    "3. Machine Translation\n",
    "4. Chatbots/Dialog Systems\n",
    "5. Question Answering Systems\n",
    "etc..\n",
    "\n",
    "However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length.\n",
    "\n",
    "### **Why NLP is hard?**\n",
    "1. Complexity of representation\n",
    "> Poems, Sarcasm, etc...  \n",
    "> Example 1: This task is a piece of cake.  \n",
    "> Example 2: You have a football game tomorrow. Break a leg!\n",
    "\n",
    "2. Ambiguity in Natural Language\n",
    "> Ambiguity means uncertainity of meaning.  \n",
    "> For Example: The car hit the pole while it was moving.\n",
    "\n",
    "### **Vectorization Techniques (AKA Feature Engineering or Extraction - Convert Text to Numerical Vectors)**\n",
    "\n",
    "1. Bag of Words\n",
    "2. TF IDF (Term Frequency - Inverse Document Frequency)\n",
    "3. Word2Vec (by Google)\n",
    "4. GloVe (Global Vectors by Stanford)\n",
    "5. FastText (by Facebook)\n",
    "6. ELMo (Embeddings from Language Models)\n",
    "7. GPT (Generative Pre-trained Transformer by OpenAI)\n",
    "8. BERT (Bidirectional Encoder Representations from Transformer by Google)\n",
    "9. LLM's\n",
    "\n",
    "**Only the following technniques are covered in this notebook:**\n",
    "1. Bag of Words\n",
    "2. TF IDF (Term Frequency - Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ft7-3UH5SfZe"
   },
   "source": [
    "## **Bag of Word Representation**\n",
    "\n",
    "We call vectorization the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the Bag of Words or \"Bag of n-grams\" representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document.\n",
    "\n",
    "\n",
    "\n",
    "We will use `CountVectorizer` to **convert text into a matrix of token count**.\n",
    "\n",
    "**We are going to perform below mentioned steps to understand the entire process:**  \n",
    "a. Converting text to numerical vectors with the help of `CountVectorizer`  \n",
    "b. Understand `fit` and `transform`  \n",
    "c. Looking at `vocabulary_`  \n",
    "d. Converting sparse matrix to dense matrix using `toarray()`  \n",
    "e. Understanding `n_gram`  \n",
    "\n",
    "### **Advantages**\n",
    "1. It is simple to understand and implement like OneHotEncoding.\n",
    "2. We have a fixed length encoding for any sequence of arbitrary length.\n",
    "3. Documents with same words/vocabulary will have similar representation. So if two documents have a similar vocabulary, they’ll be closer to each other in the vector space and vice versa.\n",
    "\n",
    "### **Disadvantages**\n",
    "1. The size of vector increases with the size of the vocabulary. Thus, sparsity continues to be a problem. One way to control it is by limiting the vocabulary to n number of the most frequent words.\n",
    "2. It does not capture the similarity between different words that mean the same thing. i.e. Semantic Meaning is not captured.\n",
    "> a. \"walk\", \"walked\", and \"walking\". BoW vectors of all three tokens will be equally apart.  \n",
    "> b. \"search\" and \"explore\" are synonyms. BoW won't capture the semantic similarity of these words.\n",
    "3. This representation does not have any way to handle out of vocabulary (OOV) words (i.e., new words that were not seen in the corpus that was used to build the vectorizer).\n",
    "4. As the name indicates, it is a “bag” of words. Word order information is lost in this representation. One way to control it is by using n-grams.\n",
    "5. It suffers from **curse of high dimensionality.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **a. BoW Text Vectorization: Apply CountVectorizer**\n",
    "\n",
    "**Parameters**\n",
    "1. encoding: str, default='utf-8'\n",
    "2. decode_error: {'strict', 'ignore', 'replace'}, default='strict' \n",
    "    - Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given `encoding`.\n",
    "3. token_pattern: str or None, default=r\"(?u)\\b\\w\\w+\\b\"\n",
    "    - Regular expression denoting what constitutes a \"token\". The default regexp select tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator). Let's break down this default pattern:\n",
    "        - **(?u)**: This flag makes the pattern Unicode aware.\n",
    "        - **\\b**: Word boundary. It prevents partial matching of pattern in long sequences.\n",
    "        - **\\w\\w+**: Match any word character (equivalent to [a-zA-Z0-9_]), at least two times (\\w means a single word character, and \\w+ means one or more word characters).\n",
    "        - **\\b**: Another word boundary.\n",
    "    - If there is a capturing group in token_pattern then the captured group content, not the entire match, becomes the token. At most one capturing group is permitted.\n",
    "4. tokenizer: callable, default=None\n",
    "    - Override the string tokenization step while preserving the preprocessing and n-grams generation steps.\n",
    "5. ngram_range: tuple (min_n, max_n), default=(1, 1)\n",
    "    - The lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted. All values of n such such that min_n <= n <= max_n will be used. For example an ngram_range of (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams.\n",
    "6. strip_accents: {'ascii', 'unicode'}, default=None\n",
    "    - Remove accents and perform other character normalization during the preprocessing step.\n",
    "7. lowercase: bool, default=True\n",
    "    - Convert all characters to lowercase before tokenizing.\n",
    "8. preprocessor: callable, default=None\n",
    "    - Override the preprocessing (strip_accents and lowercase) stage while preserving the tokenizing and n-grams generation steps.\n",
    "9. stop_words: {'english'}, a list, default=None\n",
    "    - 'english': Use a built-in list of English stop words.\n",
    "    - A list: Provide your own list of stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (3, 8)\n",
      "Type of Numerical Representation: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Vocabulary learned: ['123' 'and' 'is' 'numbers' 'symbols' 'test' 'this' 'with']\n"
     ]
    }
   ],
   "source": [
    "# Import the CountVectorizer i.e. Bag of Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text = [\"This is a test\", \"with numbers 123\", \"and symbols #!\"]\n",
    "\n",
    "# initialize the object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform\n",
    "num_rep = vectorizer.fit_transform(text)\n",
    "\n",
    "print(\"Shape:\", num_rep.shape)\n",
    "print(\"Type of Numerical Representation:\", type(num_rep))\n",
    "print(\"Vocabulary learned:\", vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Important Observation:**  \n",
    "1. By default CountVectorizer took care of removing special characters.\n",
    "2. It only reads alphanumeric characters and tokenize only if the length is greater than or equal to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We are Learning Machine Learning $</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Processing natural - language data.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10 machine - learning algorithms.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we Are Mimicing natural intelligence</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   text\n",
       "0    We are Learning Machine Learning $\n",
       "1   Processing natural - language data.\n",
       "2     10 machine - learning algorithms.\n",
       "3  we Are Mimicing natural intelligence"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "lst_text = [\"We are Learning Machine Learning $\", \n",
    "            \"Processing natural - language data.\", \n",
    "            \"10 machine - learning algorithms.\", \n",
    "            \"we Are Mimicing natural intelligence\"]\n",
    "\n",
    "df = pd.DataFrame({'text': lst_text})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the next section: Custom Text Cleaning\n",
    "# we will study a problem with this approach\n",
    "def tokenizer(doc):\n",
    "    return doc.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "sm6L-q4cSfZg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DTM (# of docs, # of unique vocabulary): (4, 18)\n",
      "Type of DTM (i.e. Compressed Sparse Row (CSR) format): <class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# Bag of Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.\n",
    "bow_vect = CountVectorizer(token_pattern=None,\n",
    "                           tokenizer=tokenizer,\n",
    "                           ngram_range=(1, 1), \n",
    "                           lowercase=False, \n",
    "                           preprocessor=None, \n",
    "                           stop_words=None)\n",
    "\n",
    "# fit_transform() does two functions: \n",
    "# First, it fits and learns the vocabulary \n",
    "# Second, it transforms our training data into feature vectors\n",
    "# The input to fit_transform should be a list of strings\n",
    "dtm = bow_vect.fit_transform(df['text'])\n",
    "\n",
    "print(f\"Shape of DTM (# of docs, # of unique vocabulary): {dtm.shape}\")\n",
    "\n",
    "print(f\"Type of DTM (i.e. Compressed Sparse Row (CSR) format): {type(dtm)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x18 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 20 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNjyY1QySfZk"
   },
   "source": [
    "#### **A note on Sparsity**\n",
    "As most documents will typically use a very small subset of the words used in the corpus, the resulting matrix will have many feature values that are zeros (typically more than 99% of them).\n",
    "\n",
    "For instance a collection of 10,000 short text documents (such as emails) will use a vocabulary with a size in the order of 100,000 unique words in total while each document will use 100 to 1000 unique words individually.\n",
    "\n",
    "In order to be able to store such a matrix in memory but also to speed up algebraic operations matrix / vector, implementations will typically use a sparse representation such as the implementations available in the scipy.sparse package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zpfrc21bSfZg",
    "outputId": "28abe17e-4992-4eb5-d88d-365baadca6a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 18\n",
      "\n",
      "Let's look at the vocabulary stored in the object: \n",
      "{'We': 8, 'are': 10, 'Learning': 4, 'Machine': 5, '$': 0, 'Processing': 7, 'natural': 16, '-': 1, 'language': 13, 'data.': 11, '10': 2, 'machine': 15, 'learning': 14, 'algorithms.': 9, 'we': 17, 'Are': 3, 'Mimicing': 6, 'intelligence': 12}\n",
      "\n",
      "Output Feature Names: ['$' '-' '10' 'Are' 'Learning' 'Machine' 'Mimicing' 'Processing' 'We'\n",
      " 'algorithms.' 'are' 'data.' 'intelligence' 'language' 'learning'\n",
      " 'machine' 'natural' 'we']\n"
     ]
    }
   ],
   "source": [
    "# We can look at unique words by using 'vocabulary_'\n",
    "\n",
    "print(f\"Vocabulary size: {len(bow_vect.vocabulary_)}\")\n",
    "print()\n",
    "print(f\"Let's look at the vocabulary stored in the object: \\n{bow_vect.vocabulary_}\")\n",
    "print()\n",
    "print(\"Output Feature Names:\", bow_vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 2 1 0 0 1 0 1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0]\n",
      " [0 1 1 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0]\n",
      " [0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# Since the dtm is sparse, lets convert it into numpy array\n",
    "\n",
    "print(dtm.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P4RsgNj2SfZh",
    "outputId": "383378b0-7067-4fef-b27b-f1c2b514c19a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 4)\t2\n",
      "  (0, 5)\t1\n",
      "  (0, 0)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 16)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 13)\t1\n",
      "  (1, 11)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 15)\t1\n",
      "  (2, 14)\t1\n",
      "  (2, 9)\t1\n",
      "  (3, 16)\t1\n",
      "  (3, 17)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 6)\t1\n",
      "  (3, 12)\t1\n"
     ]
    }
   ],
   "source": [
    "# Lets look at the Compressed Sparse Row (CSR) format\n",
    "\n",
    "print(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "jHFHo3o8SfZi",
    "outputId": "225e0bbf-875f-405f-a5a4-640f7634363f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>$</th>\n",
       "      <th>-</th>\n",
       "      <th>10</th>\n",
       "      <th>Are</th>\n",
       "      <th>Learning</th>\n",
       "      <th>Machine</th>\n",
       "      <th>Mimicing</th>\n",
       "      <th>Processing</th>\n",
       "      <th>We</th>\n",
       "      <th>algorithms.</th>\n",
       "      <th>are</th>\n",
       "      <th>data.</th>\n",
       "      <th>intelligence</th>\n",
       "      <th>language</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>natural</th>\n",
       "      <th>we</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   $  -  10  Are  Learning  Machine  Mimicing  Processing  We  algorithms.  \\\n",
       "0  1  0   0    0         2        1         0           0   1            0   \n",
       "1  0  1   0    0         0        0         0           1   0            0   \n",
       "2  0  1   1    0         0        0         0           0   0            1   \n",
       "3  0  0   0    1         0        0         1           0   0            0   \n",
       "\n",
       "   are  data.  intelligence  language  learning  machine  natural  we  \n",
       "0    1      0             0         0         0        0        0   0  \n",
       "1    0      1             0         1         0        0        1   0  \n",
       "2    0      0             0         0         1        1        0   0  \n",
       "3    0      0             1         0         0        0        1   1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the sparse matrix to a dataframe\n",
    "\n",
    "pd.DataFrame(dtm.toarray(), \n",
    "             columns=bow_vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **b. BoW Text Vectorization: Apply CountVectorizer with `ngram_range=(1,2)` and `lowercase=False`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "x6z37rArSfZj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DTM (# of docs, # of unique vocabulary): (4, 35)\n",
      "Type of DTM (i.e. Compressed Sparse Row (CSR) format): <class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bow_vect = CountVectorizer(token_pattern=None,\n",
    "                           tokenizer=token,\n",
    "                           ngram_range=(1, 2), \n",
    "                           lowercase=False, \n",
    "                           preprocessor=None, \n",
    "                           stop_words=None)\n",
    "\n",
    "dtm = bow_vect.fit_transform(df['text'])\n",
    "\n",
    "print(f\"Shape of DTM (# of docs, # of unique vocabulary): {dtm.shape}\")\n",
    "\n",
    "print(f\"Type of DTM (i.e. Compressed Sparse Row (CSR) format): {type(dtm)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x35 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 37 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ShdbxBF-SfZj",
    "outputId": "e2090759-db69-43e2-c614-167650facb58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 35\n",
      "\n",
      "Let's look at the vocabulary stored in the object: \n",
      "{'We': 17, 'are': 20, 'Learning': 8, 'Machine': 11, '$': 0, 'We are': 18, 'are Learning': 21, 'Learning Machine': 10, 'Machine Learning': 12, 'Learning $': 9, 'Processing': 15, 'natural': 30, '-': 1, 'language': 24, 'data.': 22, 'Processing natural': 16, 'natural -': 31, '- language': 2, 'language data.': 25, '10': 4, 'machine': 28, 'learning': 26, 'algorithms.': 19, '10 machine': 5, 'machine -': 29, '- learning': 3, 'learning algorithms.': 27, 'we': 33, 'Are': 6, 'Mimicing': 13, 'intelligence': 23, 'we Are': 34, 'Are Mimicing': 7, 'Mimicing natural': 14, 'natural intelligence': 32}\n",
      "\n",
      "Output Feature Names: ['$' '-' '- language' '- learning' '10' '10 machine' 'Are' 'Are Mimicing'\n",
      " 'Learning' 'Learning $' 'Learning Machine' 'Machine' 'Machine Learning'\n",
      " 'Mimicing' 'Mimicing natural' 'Processing' 'Processing natural' 'We'\n",
      " 'We are' 'algorithms.' 'are' 'are Learning' 'data.' 'intelligence'\n",
      " 'language' 'language data.' 'learning' 'learning algorithms.' 'machine'\n",
      " 'machine -' 'natural' 'natural -' 'natural intelligence' 'we' 'we Are']\n"
     ]
    }
   ],
   "source": [
    "# We can look at unique words by using 'vocabulary_'\n",
    "\n",
    "print(f\"Vocabulary size: {len(bow_vect.vocabulary_)}\")\n",
    "print()\n",
    "print(f\"Let's look at the vocabulary stored in the object: \\n{bow_vect.vocabulary_}\")\n",
    "print()\n",
    "print(\"Output Feature Names:\", bow_vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LUB8kQ39SfZj",
    "outputId": "4893a434-ebcd-4a40-bd06-eb0ead284f77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0 0 0 0 2 1 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 0]\n",
      " [0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# convert sparse matrix to numpy array\n",
    "print(dtm.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "TFIswN7TSfZk",
    "outputId": "323138bf-7e5c-42a1-8f2e-b29bdce2e221"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>$</th>\n",
       "      <th>-</th>\n",
       "      <th>- language</th>\n",
       "      <th>- learning</th>\n",
       "      <th>10</th>\n",
       "      <th>10 machine</th>\n",
       "      <th>Are</th>\n",
       "      <th>Are Mimicing</th>\n",
       "      <th>Learning</th>\n",
       "      <th>Learning $</th>\n",
       "      <th>...</th>\n",
       "      <th>language data.</th>\n",
       "      <th>learning</th>\n",
       "      <th>learning algorithms.</th>\n",
       "      <th>machine</th>\n",
       "      <th>machine -</th>\n",
       "      <th>natural</th>\n",
       "      <th>natural -</th>\n",
       "      <th>natural intelligence</th>\n",
       "      <th>we</th>\n",
       "      <th>we Are</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   $  -  - language  - learning  10  10 machine  Are  Are Mimicing  Learning  \\\n",
       "0  1  0           0           0   0           0    0             0         2   \n",
       "1  0  1           1           0   0           0    0             0         0   \n",
       "2  0  1           0           1   1           1    0             0         0   \n",
       "3  0  0           0           0   0           0    1             1         0   \n",
       "\n",
       "   Learning $  ...  language data.  learning  learning algorithms.  machine  \\\n",
       "0           1  ...               0         0                     0        0   \n",
       "1           0  ...               1         0                     0        0   \n",
       "2           0  ...               0         1                     1        1   \n",
       "3           0  ...               0         0                     0        0   \n",
       "\n",
       "   machine -  natural  natural -  natural intelligence  we  we Are  \n",
       "0          0        0          0                     0   0       0  \n",
       "1          0        1          1                     0   0       0  \n",
       "2          1        0          0                     0   0       0  \n",
       "3          0        1          0                     1   1       1  \n",
       "\n",
       "[4 rows x 35 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the sparse matrix to a dataframe\n",
    "\n",
    "pd.DataFrame(dtm.toarray(), \n",
    "             columns=bow_vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6mYc-Zk2SfZl"
   },
   "source": [
    "## **Term Frequency - Inverse Document Frequency (TF IDF)**\n",
    "\n",
    "In BOW approach all the words in the text are treated as equally important i.e. there's no notion of some words in the document being more important than others. TF-IDF, or term frequency-inverse document frequency, addresses this issue. It aims to quantify the importance of a given word relative to other words in the document and in the corpus.\n",
    "\n",
    "***\n",
    "\n",
    "$$ TF \\ IDF = TF(word_i, doc_j) * IDF(word_i, corpus) $$\n",
    "\n",
    "***\n",
    "\n",
    "Let's now try to understand:\n",
    "1. **Term Frequency**\n",
    "    - Measures how frequently a term (word) appears in a document.\n",
    "\n",
    "2. **Inverse Document Frequency**\n",
    "    - Measures how important a term is within the entire corpus.\n",
    "    - It decreases the weight of terms that appear in many documents and increases the weight of terms that appear in fewer documents.\n",
    "\n",
    "***\n",
    "\n",
    "$$ TF(word_i, doc_j) = \\frac{No \\ of \\ time \\ word_i \\ occurs \\ in \\ doc_j}{Total \\ no \\ of \\ words \\ in \\ doc_j} $$\n",
    "\n",
    "$$ IDF(word_i, corpus) = \\log(\\frac{No \\ of \\ docs \\ in \\ corpus}{No \\ of \\ docs \\ which \\ contains \\ word_i}) + 1 $$\n",
    "\n",
    "**Note:** \n",
    "1. The effect of adding \"1\" to the idf in the equation above is that terms with zero idf, i.e., terms that occur in all documents in a training set, will not be entirely ignored.\n",
    "2. Also note that the idf formula above differs from the standard textbook notation that defines the idf as idf(t) = log [ n / (df(t) + 1) ]\n",
    "3. log can be base `e`, or `2` or `10`, etc...\n",
    "\n",
    "**To avoid zero division, the IDF formula is as follows:**\n",
    "$$ IDF(word_i, corpus) = \\log(\\frac{1 \\ + \\ No \\ of \\ docs \\ in \\ corpus}{1 \\ + \\ No \\ of \\ docs \\ which \\ contains \\ word_i}) + 1 $$\n",
    "\n",
    "***\n",
    "\n",
    "### **Advantages**\n",
    "1. If the word is rare in the corpus, it will be given more importance. (i.e. IDF)\n",
    "2. If the word is more frequent in a document, it will be given more importance. (i.e. TF)\n",
    "\n",
    "### **Disadvantages**\n",
    "> **Same as BOW**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TF IDF Text Vectorization: Apply TfidfVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "TVjKE9x_SfZl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output (# of docs, # of unique vocabulary): (4, 16)\n",
      "Type of output (i.e. Compressed Sparse Row (CSR) format): <class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\", \n",
    "                             ngram_range=(1, 1), \n",
    "                             lowercase=False, \n",
    "                             preprocessor=None, \n",
    "                             stop_words=None)\n",
    "\n",
    "out = tfidf_vect.fit_transform(df['text'])\n",
    "\n",
    "print(f\"Shape of output (# of docs, # of unique vocabulary): {out.shape}\")\n",
    "\n",
    "print(f\"Type of output (i.e. Compressed Sparse Row (CSR) format): {type(out)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x16 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 17 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CYKtDLK0SfZm",
    "outputId": "d192c9ff-8316-4224-fe87-75b8d8b7e878"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 16\n",
      "\n",
      "Let's look at the vocabulary stored in the object: \n",
      "{'We': 6, 'are': 8, 'Learning': 2, 'Machine': 3, 'Processing': 5, 'natural': 14, 'language': 11, 'data': 9, '10': 0, 'machine': 13, 'learning': 12, 'algorithms': 7, 'we': 15, 'Are': 1, 'Mimicing': 4, 'intelligence': 10}\n",
      "\n",
      "Output Feature Names: ['10' 'Are' 'Learning' 'Machine' 'Mimicing' 'Processing' 'We' 'algorithms'\n",
      " 'are' 'data' 'intelligence' 'language' 'learning' 'machine' 'natural'\n",
      " 'we']\n"
     ]
    }
   ],
   "source": [
    "# We can look at unique words by using 'vocabulary_'\n",
    "\n",
    "print(f\"Vocabulary size: {len(tfidf_vect.vocabulary_)}\")\n",
    "print()\n",
    "print(f\"Let's look at the vocabulary stored in the object: \\n{tfidf_vect.vocabulary_}\")\n",
    "print()\n",
    "print(\"Output Feature Names:\", tfidf_vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hqecWwkpSfZm",
    "outputId": "d789011c-d7b9-4a72-b92a-c60ab9afca42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.75592895 0.37796447 0.         0.\n",
      "  0.37796447 0.         0.37796447 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.52547275\n",
      "  0.         0.         0.         0.52547275 0.         0.52547275\n",
      "  0.         0.         0.41428875 0.        ]\n",
      " [0.5        0.         0.         0.         0.         0.\n",
      "  0.         0.5        0.         0.         0.         0.\n",
      "  0.5        0.5        0.         0.        ]\n",
      " [0.         0.46516193 0.         0.         0.46516193 0.\n",
      "  0.         0.         0.         0.         0.46516193 0.\n",
      "  0.         0.         0.36673901 0.46516193]]\n"
     ]
    }
   ],
   "source": [
    "# convert sparse matrix to nparray\n",
    "\n",
    "print(out.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "TSQ5mn5dSfZm",
    "outputId": "6a0d5b3b-9acd-4d0b-ce7d-621a5df21bbd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>Are</th>\n",
       "      <th>Learning</th>\n",
       "      <th>Machine</th>\n",
       "      <th>Mimicing</th>\n",
       "      <th>Processing</th>\n",
       "      <th>We</th>\n",
       "      <th>algorithms</th>\n",
       "      <th>are</th>\n",
       "      <th>data</th>\n",
       "      <th>intelligence</th>\n",
       "      <th>language</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>natural</th>\n",
       "      <th>we</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.755929</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.525473</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.525473</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.525473</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.414289</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.465162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.465162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.465162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.366739</td>\n",
       "      <td>0.465162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    10       Are  Learning   Machine  Mimicing  Processing        We  \\\n",
       "0  0.0  0.000000  0.755929  0.377964  0.000000    0.000000  0.377964   \n",
       "1  0.0  0.000000  0.000000  0.000000  0.000000    0.525473  0.000000   \n",
       "2  0.5  0.000000  0.000000  0.000000  0.000000    0.000000  0.000000   \n",
       "3  0.0  0.465162  0.000000  0.000000  0.465162    0.000000  0.000000   \n",
       "\n",
       "   algorithms       are      data  intelligence  language  learning  machine  \\\n",
       "0         0.0  0.377964  0.000000      0.000000  0.000000       0.0      0.0   \n",
       "1         0.0  0.000000  0.525473      0.000000  0.525473       0.0      0.0   \n",
       "2         0.5  0.000000  0.000000      0.000000  0.000000       0.5      0.5   \n",
       "3         0.0  0.000000  0.000000      0.465162  0.000000       0.0      0.0   \n",
       "\n",
       "    natural        we  \n",
       "0  0.000000  0.000000  \n",
       "1  0.414289  0.000000  \n",
       "2  0.000000  0.000000  \n",
       "3  0.366739  0.465162  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the sparse matrix to a dataframe\n",
    "\n",
    "pd.DataFrame(out.toarray(), \n",
    "             columns=tfidf_vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **End Note - How `CountVectorizer` and `TfidfVectorizer` works?**\n",
    "\n",
    "1. Apply **preprocessing** if defined. This will clean the data.\n",
    "2. As per **token_pattern** apply tokenization on the cleaned text data.\n",
    "3. Learn the unique vocabulary word after tokenization."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
