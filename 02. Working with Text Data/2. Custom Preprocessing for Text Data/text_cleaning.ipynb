{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cc5377f-7492-46e8-9ad7-37bea007e456",
   "metadata": {},
   "source": [
    "# **Working with Text Data - Text Preprocessing**\n",
    "  \n",
    "## **Text Preprocessing Steps**\n",
    "\n",
    "Text Preprocessing steps include some essential tasks to clean and remove the noise from the available data.\n",
    "\n",
    "1. **Removing Special Characters and Punctuation**\n",
    "2. **Converting to Lower Case**\n",
    "3. **Tokenization (Sentence Tokenization and Word Tokenization)**\n",
    "4. **Removing Stop Words**\n",
    "5. **Stemming or Lemmatization**\n",
    "6. **HTML Parsing and Cleanup**\n",
    "7. **Spell Correction**\n",
    "\n",
    "Note that text preprocessing is the most important step that has the implications for all other aspects of the NLP pipeline. Further, it can also be the most time-consuming part of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1532a58-2e9c-4d76-ba2b-d6e2f4d3cc2b",
   "metadata": {},
   "source": [
    "## **1. Removing Special Characters and Punctuation**\n",
    "\n",
    "Special characters like `^`, `~`, `@`, `$`, etc... Punctuations like `.`, `?`, `,`, etc...\n",
    "\n",
    "In Python **string** module contains a constant `punctuation`. It is a string containing all the punctuation characters defined by the ASCII standard. It includes characters such as `!`,`?`, `@`, `#`, etc...\n",
    "\n",
    "**A note about ASCII Standard**\n",
    "- The ASCII (American Standard Code for Information Interchange) standard is a character encoding standard used for representing text in computers and other devices that use text.\n",
    "- ASCII is a fundamental character encoding standard that assigns numerical values to characters, enabling text representation in computers.\n",
    "- ASCII is limited to 128 characters, which is insufficient for representing characters in many languages, currency symbols, emojis and for including various special symbols.\n",
    "- To address these limitations, **Unicode (i.e. UTF-8, UTF-16, etc...)** was developed as a more comprehensive standard that includes a vast array of characters from various languages and symbol sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ea89c36-ee8b-4bea-96de-35606a3b344a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "We're LaRninG1 Natural-LAnguage-Processing!üòÄ üöÄ ‚ù§Ô∏è \n",
      "In this\\ example wE are goIng to Learn variouS text9 preprocessing steps.\n",
      "I'm GoIng TO-bE Mr. Rich. ‚Çπ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_text = \"\"\"\n",
    "We're LaRninG1 Natural-LAnguage-Processing!üòÄ üöÄ ‚ù§Ô∏è \n",
    "In this\\ example wE are goIng to Learn variouS text9 preprocessing steps.\n",
    "I'm GoIng TO-bE Mr. Rich. ‚Çπ \n",
    "\"\"\"\n",
    "\n",
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01765812-132f-4fbc-ad9c-7234519ffe3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52acc169-4d30-45b7-a435-b6c4ac97e009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Were LaRninG NaturalLAnguageProcessingüòÄ üöÄ ‚ù§Ô∏è \n",
      "In this example wE are goIng to Learn variouS text preprocessing steps\n",
      "Im GoIng TObE Mr Rich ‚Çπ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's now use \"string\" module to clear up all the punctuations.\n",
    "\n",
    "text = \"\".join([char for char in raw_text if char not in string.punctuation and not char.isdigit()])\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbdb70d-0390-4fea-b17f-f7631df88428",
   "metadata": {},
   "source": [
    "### **A more powerful weapon to remove special characters and punctuations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afb21bba-aaf2-43a2-8326-24ce6d24faf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e62f4fe0-a2dc-48bd-bb3f-ab9427c4de89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " We re LaRninG  Natural LAnguage Processing!        In this  example wE are goIng to Learn variouS text  preprocessing steps. I m GoIng TO bE Mr. Rich.    \n"
     ]
    }
   ],
   "source": [
    "# Let's define a regex to match special characters and digits\n",
    "regex = \"[^a-zA-Z.!]\"\n",
    "\n",
    "text = re.sub(regex, \" \", raw_text)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012ad0e7-7d10-4135-adee-4cd9d538cadb",
   "metadata": {},
   "source": [
    "## **2. Converting to Lower Case**\n",
    "\n",
    "We convert the whole text corpus to lower case to reduce the size of the vocabulary of our text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faafeda9-f4eb-4c72-be48-50b364f9d0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " we re larning  natural language processing!        in this  example we are going to learn various text  preprocessing steps. i m going to be mr. rich.    \n"
     ]
    }
   ],
   "source": [
    "# change sentence to lower case\n",
    "\n",
    "text = text.lower()\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab1d4d5-1e3b-4a85-8dd2-f96b57570ebf",
   "metadata": {},
   "source": [
    "## **3. Tokenization (Sentence Tokenization and Word Tokenization)**\n",
    "\n",
    "This is a simple step to break the text into sentences or words.\n",
    "\n",
    "Note that in, any NLP task tokenization is one of the most important step. Hence, any NLP pipeline has to start with a reliable system to spit the text into sentences and further split a sentence into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e026fa9-437a-4df3-9288-7ddc828411b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'we', 're', 'larning', '', 'natural', 'language', 'processing!', '', '', '', '', '', '', '', 'in', 'this', '', 'example', 'we', 'are', 'going', 'to', 'learn', 'various', 'text', '', 'preprocessing', 'steps.', 'i', 'm', 'going', 'to', 'be', 'mr.', 'rich.', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "words = text.split(\" \")\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01773768-58ba-4b64-bd37-c55567f66fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' we re larning  natural language processing!        in this  example we are going to learn various text  preprocessing steps', ' i m going to be mr', ' rich', '    ']\n"
     ]
    }
   ],
   "source": [
    "sentences = text.split(\".\")\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739e4204-f8f8-46d9-98f0-e8606e8f7e8d",
   "metadata": {},
   "source": [
    "### **Introducing NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e329989-79e9-4a90-96bf-e51cc602b9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "835f1d66-e26b-4a2a-911f-4d437573d6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc8a0d9-b152-4379-8547-e1b1f9907f64",
   "metadata": {},
   "source": [
    "### **Sentence Tokenization**\n",
    "\n",
    "At surface level this might look like a very simple task. We can use a simple rule to perform sentence segmentation by breaking up text into sentences at the appearance of full stops and question marks. However, there may be abbreviations, forms of addresses **(Dr., Mr., etc.)** that may break the simple rule.\n",
    "\n",
    "Thankfully, we don't have to worry about how to solve these issues, as most NLP libraries come with some form of sentence and word splitting implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e756ae0-a0d1-482c-bb3b-a98fdbedfded",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the Punkt tokenizer models from the Natural Language Toolkit (NLTK) library\n",
    "# The Punkt tokenizer is used for sentence and word tokenization.\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f2e9592-4fe9-4cb1-82b2-b033b3a1ef43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' we re larning  natural language processing!', 'in this  example we are going to learn various text  preprocessing steps.', 'i m going to be mr. rich.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# tokenize text into sentences\n",
    "my_sentences = sent_tokenize(text)\n",
    "\n",
    "print(my_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95f5067-f3fd-46e3-aef6-6a2de3f27d07",
   "metadata": {},
   "source": [
    "### **Word Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb6f5436-89f5-44d1-9914-61951ba3bffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 're', 'larning', 'natural', 'language', 'processing', '!', 'in', 'this', 'example', 'we', 'are', 'going', 'to', 'learn', 'various', 'text', 'preprocessing', 'steps', '.', 'i', 'm', 'going', 'to', 'be', 'mr.', 'rich', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenize text to words\n",
    "words = word_tokenize(text)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3f1fce1-7ff3-4c4d-ae6f-dcbc95b52f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 're', 'larning', 'natural', 'language', 'processing', '!']\n",
      "['in', 'this', 'example', 'we', 'are', 'going', 'to', 'learn', 'various', 'text', 'preprocessing', 'steps', '.']\n",
      "['i', 'm', 'going', 'to', 'be', 'mr.', 'rich', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenize sentences to words\n",
    "for sentence in sent_tokenize(text):\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0a730d-ee32-408b-99c2-98c27b15399c",
   "metadata": {},
   "source": [
    "## **4. Removing Stop Words**\n",
    "\n",
    "Stopwords don't contribute to the meaning of a sentence. So, we can safely remove them without changing the meaning of the sentence. For eg: a, an, the, was, is, by, etc are the stopwords.\n",
    "\n",
    "Such words are called **stop words** and are typically(though not always) removed from further analysis. There is no standard list of stop words for English. There are some popular lists for example, NLTK has one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b059c368-225e-47a7-bda2-9f0a97bcfeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6b7733c-24ac-4a27-8124-1263794ac3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the stop words corpus\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85731e38-1da7-4bae-9d90-43ff08594516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of stopwords:\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(\"List of stopwords:\")\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e31d4506-4a13-4a7e-ae07-508b09e47cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['larning', 'natural', 'language', 'processing', '!', 'example', 'going', 'learn', 'various', 'text', 'preprocessing', 'steps', '.', 'going', 'mr.', 'rich', '.']\n"
     ]
    }
   ],
   "source": [
    "# Removing stop words\n",
    "\n",
    "words = [word for word in words if word not in stopwords.words(\"english\")]\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613d44a9-8183-4bdf-8ba2-5814052661ed",
   "metadata": {},
   "source": [
    "## **5. Stemming**\n",
    "\n",
    "Stemming is the process of removing suffixes and reducing a word to some root form such that all different variants of that word can be represented by the same form. For eg: warm, warmer, warming can be converted to warm.\n",
    "\n",
    "This is accomplished by applying a fixed set of rules (eg: if the word ends with \"-es\", remove \"-es\".\n",
    "\n",
    "Stemming is commonly used in text classification to reduce the feature space to train machine learning models.\n",
    "\n",
    "Popular stemming techniques are:\n",
    "1. Porter Stemmer\n",
    "2. Snowball Stemmer (AKA Porter2)\n",
    "3. Lancaster Stemmer (Fastest approach, but not advised to use.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca294ac6-e8e2-4ff0-9968-96fb0d7d77ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['larn', 'natur', 'languag', 'process', '!', 'exampl', 'go', 'learn', 'variou', 'text', 'preprocess', 'step', '.', 'go', 'mr.', 'rich', '.']\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "## initialise the inbuilt Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "clean_tokens_stem = [stemmer.stem(word) for word in words]\n",
    "\n",
    "print(clean_tokens_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cacd601-9828-4715-a75c-a5569366a2dd",
   "metadata": {},
   "source": [
    "## **6. Lemmatization**\n",
    "\n",
    "Lemmatization is the process of mapping all the different forms of a word to its base word. While this seems close to the definition of stemming, they are, in fact, different. Lemmatization uses more linguistic knowledge to keep output human readable.\n",
    "\n",
    "To implement NLTK Lemmatizer, we must download 'wordnet' and 'omw-1.4'.\n",
    "1. **wordnet** - WordNet is a large lexical database of English. By downloading WordNet, you gain access to a comprehensive database that can be used to understand word meanings, relationships, and hierarchies.\n",
    "2. **omw-1.4** - The Open Multilingual WordNet (OMW) is a collection of wordnets in various languages. It provides translations and multilingual synsets, which are useful for cross-linguistic NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6950dd1-96b2-4467-bb6d-d4f6fea62189",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading wordnet before applying Lemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d55dfa95-4661-4c47-880e-e84217abe0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['larning', 'natural', 'language', 'processing', '!', 'example', 'going', 'learn', 'various', 'text', 'preprocessing', 'step', '.', 'going', 'mr.', 'rich', '.']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "## We can also use Lemmatizer instead of Stemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "clean_tokens_lem = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "print(clean_tokens_lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520e6c26-8682-486d-8d38-51d93e595d75",
   "metadata": {},
   "source": [
    "## **Putting all the steps together**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52f66d91-35ab-42bb-9aa7-c30363b42927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We are Learning Machine Learning $</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Processing natural - language data.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10 machine - learning algorithms.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we Are Mimicing natural intelligence</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   text\n",
       "0    We are Learning Machine Learning $\n",
       "1   Processing natural - language data.\n",
       "2     10 machine - learning algorithms.\n",
       "3  we Are Mimicing natural intelligence"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "lst_text = [\"We are Learning Machine Learning $\", \n",
    "            \"Processing natural - language data.\", \n",
    "            \"10 machine - learning algorithms.\", \n",
    "            \"we Are Mimicing natural intelligence\"]\n",
    "\n",
    "df = pd.DataFrame({'text': lst_text})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b566615c-9da2-4add-a800-2ecc14d0a00c",
   "metadata": {},
   "source": [
    "### **Explicitly Cleaning the Text and Vectorizing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10dc977c-6536-4b35-b442-3c4f66a90512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def clean(doc): \n",
    "    # doc is a string of text\n",
    "    \n",
    "    # Let's define a regex to match special characters and digits\n",
    "    regex = \"[^a-zA-Z.]\"\n",
    "    doc = re.sub(regex, \" \", doc)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    doc = doc.lower()\n",
    "        \n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "\n",
    "    # Stop word removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    \n",
    "    # Join and return\n",
    "    return \" \".join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e9e1056-ac5a-4fbf-94d4-d0286f0f0baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We are Learning Machine Learning $</td>\n",
       "      <td>learning machine learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Processing natural - language data.</td>\n",
       "      <td>processing natural language data .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10 machine - learning algorithms.</td>\n",
       "      <td>machine learning algorithm .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we Are Mimicing natural intelligence</td>\n",
       "      <td>mimicing natural intelligence</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   text                          clean_text\n",
       "0    We are Learning Machine Learning $           learning machine learning\n",
       "1   Processing natural - language data.  processing natural language data .\n",
       "2     10 machine - learning algorithms.        machine learning algorithm .\n",
       "3  we Are Mimicing natural intelligence       mimicing natural intelligence"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text'] = df['text'].apply(lambda x : clean(x))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74e94857-d878-4ed6-9cd9-191e24d9568f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of text_dtm (# of docs, # of unique vocabulary): (4, 9)\n",
      "Vocab: ['algorithm' 'data' 'intelligence' 'language' 'learning' 'machine'\n",
      " 'mimicing' 'natural' 'processing']\n"
     ]
    }
   ],
   "source": [
    "# import feature extraction methods from sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# instantiate a vectoriezer\n",
    "bow_vect = CountVectorizer()\n",
    "\n",
    "# use it to extract features from training data\n",
    "text_dtm = bow_vect.fit_transform(df['clean_text'])\n",
    "\n",
    "print()\n",
    "print(f\"Shape of text_dtm (# of docs, # of unique vocabulary): {text_dtm.shape}\")\n",
    "print(f\"Vocab: {bow_vect.get_feature_names_out()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8958f08a-60f3-4d93-9500-9ab46959f275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algorithm</th>\n",
       "      <th>data</th>\n",
       "      <th>intelligence</th>\n",
       "      <th>language</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>mimicing</th>\n",
       "      <th>natural</th>\n",
       "      <th>processing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   algorithm  data  intelligence  language  learning  machine  mimicing  \\\n",
       "0          0     0             0         0         2        1         0   \n",
       "1          0     1             0         1         0        0         0   \n",
       "2          1     0             0         0         1        1         0   \n",
       "3          0     0             1         0         0        0         1   \n",
       "\n",
       "   natural  processing  \n",
       "0        0           0  \n",
       "1        1           1  \n",
       "2        0           0  \n",
       "3        1           0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(text_dtm.toarray(), \n",
    "            columns=bow_vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b62fdf7-5ff2-4329-b719-e5139e1f53a2",
   "metadata": {},
   "source": [
    "##### **Observe that full stop is removed by CountVectorizer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ced0276-193f-4c71-9d76-518ae6c52575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(doc):\n",
    "    return nltk.word_tokenize(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b59271fb-e4b6-4b7c-afcb-5a84bc4dd592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of text_dtm (# of docs, # of unique vocabulary): (4, 10)\n",
      "Vocab: ['.' 'algorithm' 'data' 'intelligence' 'language' 'learning' 'machine'\n",
      " 'mimicing' 'natural' 'processing']\n"
     ]
    }
   ],
   "source": [
    "# import feature extraction methods from sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# instantiate a vectoriezer\n",
    "bow_vect = CountVectorizer(token_pattern=None,\n",
    "                           tokenizer=tokenizer, \n",
    "                           ngram_range=(1, 1), \n",
    "                           lowercase=False)\n",
    "\n",
    "# use it to extract features from training data\n",
    "text_dtm = bow_vect.fit_transform(df['clean_text'])\n",
    "\n",
    "print()\n",
    "print(f\"Shape of text_dtm (# of docs, # of unique vocabulary): {text_dtm.shape}\")\n",
    "print(f\"Vocab: {bow_vect.get_feature_names_out()[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd863ee1-823a-4e4e-9547-824693b6d233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>.</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>data</th>\n",
       "      <th>intelligence</th>\n",
       "      <th>language</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>mimicing</th>\n",
       "      <th>natural</th>\n",
       "      <th>processing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   .  algorithm  data  intelligence  language  learning  machine  mimicing  \\\n",
       "0  0          0     0             0         0         2        1         0   \n",
       "1  1          0     1             0         1         0        0         0   \n",
       "2  1          1     0             0         0         1        1         0   \n",
       "3  0          0     0             1         0         0        0         1   \n",
       "\n",
       "   natural  processing  \n",
       "0        0           0  \n",
       "1        1           1  \n",
       "2        0           0  \n",
       "3        1           0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(text_dtm.toarray(), \n",
    "            columns=bow_vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bd60f2-3202-480d-9b80-5accbc94de2e",
   "metadata": {},
   "source": [
    "### **Implicitly Cleaning the Text during Vectorization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "58a85148-619b-4b50-876a-419b41fe1b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(doc):\n",
    "    return nltk.word_tokenize(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef496284-4bbc-47a9-993b-052cd187c097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of text_dtm (# of docs, # of unique vocabulary): (4, 10)\n",
      "Vocab: ['.' 'algorithm' 'data' 'intelligence' 'language' 'learning' 'machine'\n",
      " 'mimicing' 'natural' 'processing']\n"
     ]
    }
   ],
   "source": [
    "# import feature extraction methods from sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# instantiate a vectoriezer\n",
    "bow_vect = CountVectorizer(token_pattern=None,\n",
    "                           tokenizer=tokenizer, \n",
    "                           ngram_range=(1, 1), \n",
    "                           lowercase=False, \n",
    "                           preprocessor=clean, \n",
    "                           stop_words=None)\n",
    "\n",
    "# use it to extract features from training data\n",
    "text_dtm = bow_vect.fit_transform(df['text'])\n",
    "\n",
    "print()\n",
    "print(f\"Shape of text_dtm (# of docs, # of unique vocabulary): {text_dtm.shape}\")\n",
    "print(f\"Vocab: {bow_vect.get_feature_names_out()[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b178268e-a515-4960-94c7-afcb243e2d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>.</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>data</th>\n",
       "      <th>intelligence</th>\n",
       "      <th>language</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>mimicing</th>\n",
       "      <th>natural</th>\n",
       "      <th>processing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   .  algorithm  data  intelligence  language  learning  machine  mimicing  \\\n",
       "0  0          0     0             0         0         2        1         0   \n",
       "1  1          0     1             0         1         0        0         0   \n",
       "2  1          1     0             0         0         1        1         0   \n",
       "3  0          0     0             1         0         0        0         1   \n",
       "\n",
       "   natural  processing  \n",
       "0        0           0  \n",
       "1        1           1  \n",
       "2        0           0  \n",
       "3        1           0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the sparse matrix to a dataframe\n",
    "\n",
    "pd.DataFrame(text_dtm.toarray(), \n",
    "             columns=bow_vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7230a865-35d8-475a-9eee-ee8d226c25dc",
   "metadata": {},
   "source": [
    "## **More Custom Text Cleaning Steps**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbb29fc-a0bb-4927-b322-abe63e73d8ad",
   "metadata": {},
   "source": [
    "### **Spell Corrector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec29a4a2-ee70-446c-b950-0b5fb84b908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d0f2d3c-c93e-4449-9aba-f37e0c475476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:  we re larning  natural language processing!        in this  example we are going to learn various text  preprocessing steps. i m going to be mr. rich.    \n",
      "\n",
      "Corrected text:  we re learning  natural language processing!        in this  example we are going to learn various text  preprocessing steps. i m going to be mr. rich.    \n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Define a text with spelling errors\n",
    "raw_text = \"\"\"\n",
    "We're LaRninG1 Natural-LAnguage-Processing!üòÄ üöÄ ‚ù§Ô∏è \n",
    "In this\\ example wE are goIng to Learn variouS text9 preprocessing steps.\n",
    "I'm GoIng TO-bE Mr. Rich. ‚Çπ \n",
    "\"\"\"\n",
    "\n",
    "# Create a TextBlob object\n",
    "blob = TextBlob(text)\n",
    "\n",
    "# Correct the spelling\n",
    "corrected_text = blob.correct()\n",
    "\n",
    "print(\"Original text:\", text)\n",
    "print()\n",
    "print(\"Corrected text:\", corrected_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797f5dc9-6b93-4fcd-9d7e-c11d32a557a1",
   "metadata": {},
   "source": [
    "### **HTML Parsing and Cleanup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0964f8d6-7e43-4f6d-b3eb-9de8933e9a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning Wiki Page Source\n",
    "\n",
    "data = \"\"\"\n",
    "<figure class=\"mw-default-size\" typeof=\"mw:File/Thumb\">\n",
    "    <a href=\"/wiki/File:AI_hierarchy.svg\" class=\"mw-file-description\"><img src=\"dummy_src\" /></a>\n",
    "    <figcaption>Machine learning as subfield of AI<sup id=\"cite_ref-journalimcms.org_22-0\" class=\"reference\">\n",
    "    <a href=\"#cite_note-journalimcms.org-22\">&#91;22&#93;</a></sup></figcaption>\n",
    "</figure>\n",
    "<p>As a scientific endeavor, machine learning grew out of the quest for <a href=\"/wiki/Artificial_intelligence\" title=\"Artificial intelligence\">artificial intelligence</a> (AI). In the early days of AI as an <a href=\"/wiki/Discipline_(academia)\" class=\"mw-redirect\" title=\"Discipline (academia)\">academic discipline</a>, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed \"<a href=\"/wiki/Artificial_neural_network\" class=\"mw-redirect\" title=\"Artificial neural network\">neural networks</a>\"; these were mostly <a href=\"/wiki/Perceptron\" title=\"Perceptron\">perceptrons</a> and <a href=\"/wiki/ADALINE\" title=\"ADALINE\">other models</a> that were later found to be reinventions of the <a href=\"/wiki/Generalized_linear_model\" title=\"Generalized linear model\">generalized linear models</a> of statistics.<sup id=\"cite_ref-23\" class=\"reference\"><a href=\"#cite_note-23\">&#91;23&#93;</a></sup> <a href=\"/wiki/Probabilistic_reasoning\" class=\"mw-redirect\" title=\"Probabilistic reasoning\">Probabilistic reasoning</a> was also employed, especially in <a href=\"/wiki/Automated_medical_diagnosis\" class=\"mw-redirect\" title=\"Automated medical diagnosis\">automated medical diagnosis</a>.<sup id=\"cite_ref-aima_24-0\" class=\"reference\"><a href=\"#cite_note-aima-24\">&#91;24&#93;</a></sup><sup class=\"reference nowrap\"><span title=\"Page / location: 488\">&#58;&#8202;488&#8202;</span></sup></p>\n",
    "<p>However, an increasing emphasis on the <a href=\"/wiki/Symbolic_AI\" class=\"mw-redirect\" title=\"Symbolic AI\">logical, knowledge-based approach</a> caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.<sup id=\"cite_ref-aima_24-1\" class=\"reference\"><a href=\"#cite_note-aima-24\">&#91;24&#93;</a></sup><sup class=\"reference nowrap\"><span title=\"Page / location: 488\">&#58;&#8202;488&#8202;</span></sup> By 1980, <a href=\"/wiki/Expert_system\" title=\"Expert system\">expert systems</a> had come to dominate AI, and statistics was out of favor.<sup id=\"cite_ref-changing_25-0\" class=\"reference\"><a href=\"#cite_note-changing-25\">&#91;25&#93;</a></sup> Work on symbolic/knowledge-based learning did continue within AI, leading to <a href=\"/wiki/Inductive_logic_programming\" title=\"Inductive logic programming\">inductive logic programming</a>(ILP), but the more statistical line of research was now outside the field of AI proper, in <a href=\"/wiki/Pattern_recognition\" title=\"Pattern recognition\">pattern recognition</a> and <a href=\"/wiki/Information_retrieval\" title=\"Information retrieval\">information retrieval</a>.<sup id=\"cite_ref-aima_24-2\" class=\"reference\"><a href=\"#cite_note-aima-24\">&#91;24&#93;</a></sup><sup class=\"reference nowrap\"><span title=\"Page / location: 708‚Äì710, 755\">&#58;&#8202;708‚Äì710,&#8202;755&#8202;</span></sup> Neural networks research had been abandoned by AI and <a href=\"/wiki/Computer_science\" title=\"Computer science\">computer science</a> around the same time. This line, too, was continued outside the AI/CS field, as \"<a href=\"/wiki/Connectionism\" title=\"Connectionism\">connectionism</a>\", by researchers from other disciplines including <a href=\"/wiki/John_Hopfield\" title=\"John Hopfield\">Hopfield</a>, <a href=\"/wiki/David_Rumelhart\" title=\"David Rumelhart\">Rumelhart</a>, and <a href=\"/wiki/Geoff_Hinton\" class=\"mw-redirect\" title=\"Geoff Hinton\">Hinton</a>. Their main success came in the mid-1980s with the reinvention of <a href=\"/wiki/Backpropagation\" title=\"Backpropagation\">backpropagation</a>.<sup id=\"cite_ref-aima_24-3\" class=\"reference\"><a href=\"#cite_note-aima-24\">&#91;24&#93;</a></sup><sup class=\"reference nowrap\"><span title=\"Page / location: 25\">&#58;&#8202;25&#8202;</span></sup></p>\n",
    "<p>Machine learning (ML), reorganized and recognized as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the <a href=\"/wiki/Symbolic_artificial_intelligence\" title=\"Symbolic artificial intelligence\">symbolic approaches</a> it had inherited from AI, and toward methods and models borrowed from statistics, <a href=\"/wiki/Fuzzy_logic\" title=\"Fuzzy logic\">fuzzy logic</a>, and <a href=\"/wiki/Probability_theory\" title=\"Probability theory\">probability theory</a>.<sup id=\"cite_ref-changing_25-1\" class=\"reference\"><a href=\"#cite_note-changing-25\">&#91;25&#93;</a></sup></p>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "adc215d5-551f-491c-bc7b-56436a7ab8de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nMachine learning as subfield of AI\\n[22]\\n\\nAs a scientific endeavor, machine learning grew out of the quest for artificial intelligence (AI). In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed \"neural networks\"; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics.[23] Probabilistic reasoning was also employed, especially in automated medical diagnosis.[24]:\\u200a488\\u200a\\nHowever, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.[24]:\\u200a488\\u200a By 1980, expert systems had come to dominate AI, and statistics was out of favor.[25] Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming(ILP), but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.[24]:\\u200a708‚Äì710,\\u200a755\\u200a Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as \"connectionism\", by researchers from other disciplines including Hopfield, Rumelhart, and Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.[24]:\\u200a25\\u200a\\nMachine learning (ML), reorganized and recognized as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics, fuzzy logic, and probability theory.[25]\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(data)\n",
    "\n",
    "soup.get_text()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
