{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44fd72e6-594b-468f-bf1e-a18b415fe21f",
   "metadata": {},
   "source": [
    "# **Mathematical Foundations for Machine Learning - Linear Algebra**\n",
    "\n",
    "## **What's Covered?**\n",
    "1. Linear Algebra and The Concept of Vector Space\n",
    "    - How to represent a dataframe in mathematics?\n",
    "    - How to represent a datapoint (i.e dataframe row) in mathematics?\n",
    "2. Visualizing Row Vectors\n",
    "3. Properties of a Vector\n",
    "    - Dimensionality\n",
    "    - Length/Magnitude\n",
    "    - Direction \n",
    "4. Vector Algebra (in d-Dimensions)\n",
    "\t- Addition\n",
    "\t- Subtraction\n",
    "\t- Multiplication (Dot Product)\n",
    "5. Similarity between Two Vectors (Distance vs Similarity)\n",
    "    - Minkowski Distance\n",
    "    - Manhattan Distance\n",
    "    - Eucledian Distance\n",
    "    - Cosine Similarity\n",
    "    - Cosine Distance\n",
    "    - Angular Distance\n",
    "    - Dot Product\n",
    "6. Matrix Algebra Operations\n",
    "    - Addition and Subtraction\n",
    "    - Scalar Multiplication\n",
    "    - Transpose\n",
    "    - Matrix Multiplication (Dot Product)\n",
    "    - Inverse\n",
    "    - Matrix Factorization\n",
    "    - Eigenvalues and Eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f26503-562b-4d13-8db9-7484b169188f",
   "metadata": {},
   "source": [
    "## **Linear Algebra and The Concept of Vector Space**\n",
    "**Linear algebra** is a branch of mathematics that deals with vector spaces and linear mappings between these spaces. It is a fundamental tool in various fields, including physics, engineering, computer science, and economics.\n",
    "\n",
    "A vector space is like a special playground for arrows (vectors) that you can stretch, shrink, and combine in certain ways. Imagine you have different-sized arrows on a big piece of paper. This paper is our vector space. \n",
    "\n",
    "The concept of a vector space is closely related to the coordinate plane, and in fact, the coordinate plane is a fundamental example of a vector space. Coordinate plane is a two-dimensional space $ \\mathbb{R}^2 $ where each point is identified by a pair of real numbers (x, y). Whereas vector spaces generalize to spaces of higher dimensions $ \\mathbb{R}^d $.  \n",
    "\n",
    "### **Q: How to represent a dataframe in mathematics?**  \n",
    "Dataframe is represented as a numerical matrix. \n",
    "\n",
    "Each row in the DataFrame corresponds to a row in the matrix, and each column in the DataFrame corresponds to a column in the matrix. \n",
    "\n",
    "This matrix representation is useful when applying linear algebra operations or when using machine learning libraries that expect input data in matrix form. It allows you to leverage the mathematical and computational tools available for matrices in various data analysis and machine learning tasks.\n",
    "\n",
    "If your DataFrame contains a mixture of data types, it may be necessary to handle this accordingly. For eg: for categorical columns apply OHE or LE, for text columns apply BoW, TFIDF, etc...\n",
    "\n",
    "### **Q: How to represent a datapoint (i.e dataframe row) in mathematics?**  \n",
    "Remember that the DataFrame is represented as a matrix. Each row in matrix represents a vector. This vector is a matematical representation of a datapoint.\n",
    "\n",
    "A vector is a mathematical object that represents a quantity with both magnitude and direction. In the context of linear algebra, a vector is often represented as an ordered collection of numbers. The numbers in the collection correspond to the coordinates of the vector in a specific coordinate system.\n",
    "\n",
    "In two-dimensional space $ \\mathbb{R}^2 $, a vector can be represented as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\vec{\\mathbf{a}} = \\begin{bmatrix} a_1 \\\\ a_2 \\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Here, $ \\vec{\\mathbf{a}} $ is a vector, and $ a_1 $ and $ a_2 $ are its components or coordinates. The vector $ \\vec{\\mathbf{a}} $ can be visualized as an arrow starting from the origin (0, 0) and pointing to the point ($a_1, a_2$) in the coordinate plane.\n",
    "\n",
    "In three-dimensional space $ \\mathbb{R}^3 $, a vector can be represented as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\vec{\\mathbf{a}} = \\begin{bmatrix} a_1 \\\\ a_2 \\\\ a_3 \\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Each $ a_i $ represents a coordinate along a specific axis (x, y, or z).\n",
    "\n",
    "Now, regarding the representation of a vector in a matrix:\n",
    "\n",
    "1. **Row Vector:**\n",
    "   - A row vector is a vector represented as a 1 x d matrix, where d is the number of components.\n",
    "   - Example:\n",
    "\\begin{equation*}\n",
    "\\vec{\\mathbf{a}} = \\begin{bmatrix} a_1 & a_2 & a_3 \\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "2. **Column Vector (by default representation of a vector):**\n",
    "   - A column vector is a vector represented as an d x 1 matrix.\n",
    "   - Example:\n",
    "\\begin{equation*}\n",
    "\\vec{\\mathbf{a}} = \\begin{bmatrix} a_1 \\\\ a_2 \\\\ a_3 \\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "In both cases, the matrix representation captures the same information as the vector, with each element of the matrix corresponding to a component of the vector.\n",
    "\n",
    "Matrices are fundamental in linear algebra, and the representation of vectors as matrices allows for the application of matrix operations and transformations, facilitating various mathematical operations and computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3c653f-f205-427b-984d-8f7e1d0d95d1",
   "metadata": {},
   "source": [
    "## **Visualizing Row Vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5568525c-577a-47be-8fca-a77c15aae206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bd08046-91da-4f13-a913-27099ddcc5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_1 = np.array([0.5, 2])\n",
    "vec_2 = np.array([3, 0])\n",
    "vec_3 = np.array([0, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8373926-b1e5-4adb-896d-269f1060a7fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(vec_1[0], vec_1[1], 'ko')\n",
    "plt.plot(vec_2[0], vec_2[1], 'ko')\n",
    "plt.plot(vec_3[0], vec_3[1], 'ko')\n",
    "\n",
    "plt.quiver(0, 0, vec_1[0], vec_1[1], angles='xy', scale_units='xy', scale=1, color='r')\n",
    "plt.quiver(0, 0, vec_2[0], vec_2[1], angles='xy', scale_units='xy', scale=1, color='g')\n",
    "plt.quiver(0, 0, vec_3[0], vec_3[1], angles='xy', scale_units='xy', scale=1, color='b')\n",
    "\n",
    "plt.xlim(-2, 10)\n",
    "plt.ylim(-2, 10)\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a585f35-d1ba-4761-92df-983b237b2841",
   "metadata": {},
   "source": [
    "## **Properties of a Vector**\n",
    "\n",
    "\\begin{equation*}\n",
    "\\vec{\\mathbf{a}} = \\begin{bmatrix} a_1 \\\\ a_2 \\\\ a_3 \\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "### **Dimensionality**  \n",
    "The dimensionality of a vector is the number of features (columns) it has. In the above example, the dimensionality is 3.\n",
    "\n",
    "### **Magnitude of a Vector (aka Norm of a Vector)**\n",
    "- The magnitude (or length) of a vector represents the size or extent of the vector. It is always a non-negative value.\n",
    "- The magnitude of a vector $\\vec{\\mathbf{a}}$ in two-dimensional space $\\mathbb{R}^2$ with components $a_1$ and $a_2$ is calculated using the Euclidean norm (or Euclidean length) formula:\n",
    "\\begin{equation*}\n",
    "\\|\\vec{\\mathbf{a}}\\| = \\sqrt{a_1^2 + a_2^2}\n",
    "\\end{equation*}\n",
    "\n",
    "- In three-dimensional space $\\mathbb{R}^3$, for a vector $\\vec{\\mathbf{a}}$ with components $a_1$, $a_2$ and $a_3$, the magnitude is calculated as:\n",
    "\\begin{equation*}\n",
    "\\|\\vec{\\mathbf{a}}\\| = \\sqrt{a_1^2 + a_2^2 + a_3^2}\n",
    "\\end{equation*}\n",
    "\n",
    "- More generally, for a d-dimensional vector $\\vec{\\mathbf{a}} = [a_1, a_2, \\ldots, a_d]$, the magnitude is:\n",
    "\\begin{equation*}\n",
    "\\|\\vec{\\mathbf{a}}\\| = \\sqrt{a_1^2 + a_2^2 + \\ldots + a_d^2}\n",
    "\\end{equation*}\n",
    "\n",
    "### **Direction of a Vector**\n",
    "- The direction of a vector describes the way the vector points in space. \n",
    "- The direction of a vector can be expressed as a **unit vector**, which is a vector with the same direction but a magnitude of 1. It's like describing a direction without worrying about how far to go in that direction.\n",
    "- To turn a vector to unit vector, you divide each of its components by its magnitude. This is called as **Normalization**. It keeps the direction same but changes the length to 1.\n",
    "- A unit vector $\\hat{\\mathbf{u}}$ in the same direction as $\\vec{\\mathbf{a}}$ is given by:\n",
    "\\begin{equation*}\n",
    "\\hat{\\mathbf{a}} = \\frac{\\vec{\\mathbf{a}}}{\\|\\vec{\\mathbf{a}}\\|}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "796d9cf4-c434-4764-a372-0e8df91eb34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionality: (2,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimensionality:\", vec_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2ee48f1-a6bb-4858-8395-3b5b1d48bd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Magnitude(or Length) of a vec_1: 2.0615528128088303\n",
      "Magnitude(or Length) of a vec_2: 3.0\n",
      "Magnitude(or Length) of a vec_3: 8.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Magnitude(or Length) of a vec_1:\", np.linalg.norm(vec_1))\n",
    "print(\"Magnitude(or Length) of a vec_2:\", np.linalg.norm(vec_2))\n",
    "print(\"Magnitude(or Length) of a vec_3:\", np.linalg.norm(vec_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f383c9f-816e-477d-8ad1-e3c1b7ecb0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit Vector of vec_1: [0.24253563 0.9701425 ]\n",
      "Unit Vector of vec_2: [1. 0.]\n",
      "Unit Vector of vec_3: [0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Unit Vector of vec_1:\", vec_1 / np.linalg.norm(vec_1))\n",
    "print(\"Unit Vector of vec_2:\", vec_2 / np.linalg.norm(vec_2))\n",
    "print(\"Unit Vector of vec_3:\", vec_3 / np.linalg.norm(vec_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95a1fca-ec0d-48ef-acd2-58482ea849d4",
   "metadata": {},
   "source": [
    "## **Vector Algebra**\n",
    "1. Addition\n",
    "2. Subtraction\n",
    "3. Dot Product\n",
    "\n",
    "### **Addition**\n",
    "Adding two vectors of the same dimension results in another vector of the same dimension, where each component is the sum of the corresponding components of the original vectors.\n",
    "\\begin{equation*}\n",
    "\\mathbf{u} = \\begin{bmatrix} u_1 \\\\ u_2 \\\\ u_3 \\end{bmatrix}, \\quad \\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ v_3 \\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbf{u} + \\mathbf{v} = \\begin{bmatrix} u_1 + v_1 \\\\ u_2 + v_2 \\\\ u_3 + v_3 \\end{bmatrix}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88aa337e-05ab-4da1-ad71-683a8434f076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector 1: [0.5 2. ]\n",
      "Vector 2: [3 0]\n",
      "Vector 3: [0 8]\n"
     ]
    }
   ],
   "source": [
    "print(\"Vector 1:\", vec_1)\n",
    "print(\"Vector 2:\", vec_2)\n",
    "print(\"Vector 3:\", vec_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be895db8-ef26-4f7a-b498-cb05f6c50503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding vec_1 and vec_2: [3.5 2. ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Adding vec_1 and vec_2:\", vec_1 + vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3795e2-f419-42c8-8f12-47ac98211df8",
   "metadata": {},
   "source": [
    "### **Subtraction**\n",
    "Subtracting two vectors of the same dimension results in another vector of the same dimension, where each component is the difference of the corresponding components of the original vectors.\n",
    "\\begin{equation*}\n",
    "\\mathbf{u} = \\begin{bmatrix} u_1 \\\\ u_2 \\\\ u_3 \\end{bmatrix}, \\quad \\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ v_3 \\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbf{u} - \\mathbf{v} = \\begin{bmatrix} u_1 - v_1 \\\\ u_2 - v_2 \\\\ u_3 - v_3 \\end{bmatrix}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24d37465-be71-4301-ac5d-8e7920c5242e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subtraction vec_1 and vec_2: [-2.5  2. ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Subtraction vec_1 and vec_2:\", vec_1 - vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c569ce-fa0d-4729-bc0c-4fd166d68bcd",
   "metadata": {},
   "source": [
    "### **Dot Product (AKA Scalar Product)**\n",
    "The dot product of two vectors of the same dimension is a scalar value, calculated by summing the products of corresponding components.\n",
    "\n",
    "Here are several ways to express the dot product between two vectors:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\vec{\\mathbf{a}} \\cdot \\vec{\\mathbf{b}} = \\sum_{i=1}^{n} a_i b_i\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d013713f-c090-41b4-b8d5-c768f45a2f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot Product: 1.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Dot Product:\", np.dot(vec_1, vec_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f77018-05b6-4d28-b3e6-203f25b5aec7",
   "metadata": {},
   "source": [
    "## **Similarity between Two Vectors (Distance vs Similarity)**\n",
    "\n",
    "Similarity and distance are two concepts that are often used in various fields, such as mathematics, statistics, and machine learning, to quantify the relationship between datapoints. They are essentially inverse concepts, and understanding one can provide insights into the other.\n",
    "\n",
    "Similarity and distance are related but represent opposite aspects of the relationship between datapoints. High similarity corresponds to low distance, and vice versa. \n",
    "\n",
    "Different applications may require different measures, and the choice between similarity and distance depends on the specific context and requirements of the problem at hand.\n",
    "\n",
    "\n",
    "The choice of distance metric depends on the specific characteristics of your data and the requirements of your application. Different distance metrics are suitable for different types of data and scenarios. Here are some commonly used distance metrics:\n",
    "\n",
    "1. **Minkowski Distance (Lp Norm):**\n",
    "A generalization of both Euclidean and Manhattan distances. The parameter \\(p\\) allows you to vary between the two.\n",
    "\n",
    "\\begin{equation*}\n",
    "distance(\\vec{\\mathbf{a}}, \\vec{\\mathbf{b}}) = \\left(\\sum_{i=1}^{d}|a_i - b_i|^p\\right)^{1/p}\n",
    "\\end{equation*}\n",
    "\n",
    "2. **Manhattan Distance (L1 Norm)**\n",
    "3. **Euclidean Distance (L2 Norm)**\n",
    "4. **Angular Distance**\n",
    "5. **Cosine Similarity**\n",
    "6. **Cosine Distance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c3b50a-1b8c-4683-9c09-302e3529a345",
   "metadata": {},
   "source": [
    "### **Manhattan Distance (aka L1 Norm)**\n",
    "Represents the sum of the absolute differences between the coordinates. It is suitable for scenarios where movement can only occur along grid lines.\n",
    "\n",
    "\\begin{equation*}\n",
    "distance(\\vec{\\mathbf{a}}, \\vec{\\mathbf{b}}) = \\sum_{i=1}^{d}|a_i - b_i|\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cb55e48-efcd-4262-bf52-3c051b743c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manhattan Distance Between vec_1 and vec_2: [[4.5]]\n",
      "\n",
      "Manhattan Distance Between vec_1, vec_2 and vec_3:\n",
      " [[ 0.   4.5  6.5]\n",
      " [ 4.5  0.  11. ]\n",
      " [ 6.5 11.   0. ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import pairwise\n",
    "\n",
    "print(\"Manhattan Distance Between vec_1 and vec_2:\", pairwise.manhattan_distances([vec_1], [vec_2]))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Manhattan Distance Between vec_1, vec_2 and vec_3:\\n\", pairwise.manhattan_distances([vec_1, vec_2, vec_3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80be791a-3a47-4c5a-8479-f6496a6cb028",
   "metadata": {},
   "source": [
    "### **Euclidean Distance (aka L2 Norm)**\n",
    "Measures the straight-line distance between two points in Euclidean space. It is widely used and suitable for continuous data.\n",
    "\n",
    "\\begin{equation*}\n",
    "distance(\\vec{\\mathbf{a}}, \\vec{\\mathbf{b}}) = \\sqrt{\\sum_{i=1}^{d}(a_i - b_i)^2}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc73f0a8-3b94-462d-be32-f789cce6271b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean Distance Between vec_1 and vec_2: [[3.20156212]]\n",
      "\n",
      "Euclidean Distance Between vec_1, vec_2 and vec_3:\n",
      " [[0.         3.20156212 6.02079729]\n",
      " [3.20156212 0.         8.54400375]\n",
      " [6.02079729 8.54400375 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import pairwise\n",
    "\n",
    "print(\"Euclidean Distance Between vec_1 and vec_2:\", pairwise.euclidean_distances([vec_1], [vec_2]))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Euclidean Distance Between vec_1, vec_2 and vec_3:\\n\", pairwise.euclidean_distances([vec_1, vec_2, vec_3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efc0c59-2917-45b5-8f52-9b7aaa70a184",
   "metadata": {},
   "source": [
    "### **Cosine Similarity** (i.e. $\\cos\\theta$)\n",
    "Measures the cosine of the angle between two vectors. It is often used for text and high-dimensional data where the magnitude of the vectors is not important.  \n",
    "`Range:` Cosine similarity ranges from -1 to 1.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\cos\\theta_{a,b} = \\frac{\\vec{\\mathbf{a}} \\cdot \\vec{\\mathbf{b}}}{\\|\\vec{\\mathbf{a}}\\| \\times \\|\\vec{\\mathbf{b}}\\|}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ebc24ae-3857-43e7-b6d1-d44eeb6656ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Between vec_1 and vec_2: [[0.24253563]]\n",
      "\n",
      "Cosine Similarity Between vec_1, vec_2 and vec_3:\n",
      " [[1.         0.24253563 0.9701425 ]\n",
      " [0.24253563 1.         0.        ]\n",
      " [0.9701425  0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import pairwise\n",
    "\n",
    "print(\"Cosine Similarity Between vec_1 and vec_2:\", pairwise.cosine_similarity([vec_1], [vec_2]))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Cosine Similarity Between vec_1, vec_2 and vec_3:\\n\", pairwise.cosine_similarity([vec_1, vec_2, vec_3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465aafa8-2318-47a7-8ffb-44ab0ce46cec",
   "metadata": {},
   "source": [
    "### **Cosine Distance** (i.e. 1 - $\\cos\\theta$)\n",
    "It is a measure of dissimilarity, representing how far apart two vectors are in terms of orientation. A smaller cosine distance indicates greater similarity.  \n",
    "`Range:` Cosine similarity ranges from 0 to 2.\n",
    "\n",
    "\\begin{equation*}\n",
    "distance(\\mathbf{a}, \\mathbf{b}) = 1 - \\cos\\theta_{a,b}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffe3e29d-95b4-4ad2-8455-866253603013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Distance Between vec_1 and vec_2: [[0.75746437]]\n",
      "\n",
      "Cosine Distance Between vec_1, vec_2 and vec_3:\n",
      " [[0.         0.75746437 0.0298575 ]\n",
      " [0.75746437 0.         1.        ]\n",
      " [0.0298575  1.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import pairwise\n",
    "\n",
    "print(\"Cosine Distance Between vec_1 and vec_2:\", pairwise.cosine_distances([vec_1], [vec_2]))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Cosine Distance Between vec_1, vec_2 and vec_3:\\n\", pairwise.cosine_distances([vec_1, vec_2, vec_3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cc0113-45aa-476c-a5c0-dd55583df1a4",
   "metadata": {},
   "source": [
    "### **Angular Distance** (i.e. $\\theta$)\n",
    "Angular distance, also known as angular separation or angular similarity, is a measure of the separation between two vectors or points in a multi-dimensional space. It is often used in fields such as astronomy, computer vision, and machine learning to quantify the difference in orientation or direction between two entities.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\theta_{a,b} = \\cos^{-1} \\left(\\frac{\\vec{\\mathbf{a}} \\cdot \\vec{\\mathbf{b}}}{\\|\\vec{\\mathbf{a}}\\| \\times \\|\\vec{\\mathbf{b}}\\|} \\right)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a019e2f1-a7c1-4615-ae97-900b6003821f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Angular Distance Between vec_1 and vec_2: [[75.96375653]]\n",
      "\n",
      "Angular Distance Between vec_1, vec_2 and vec_3:\n",
      " [[ 0. 76. 14.]\n",
      " [76.  0. 90.]\n",
      " [14. 90.  0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import pairwise\n",
    "\n",
    "cos_sim = pairwise.cosine_similarity([vec_1], [vec_2])\n",
    "print(\"Angular Distance Between vec_1 and vec_2:\", np.degrees(np.arccos(cos_sim)) )\n",
    "\n",
    "print()\n",
    "\n",
    "cos_sim = pairwise.cosine_similarity([vec_1, vec_2, vec_3])\n",
    "print(\"Angular Distance Between vec_1, vec_2 and vec_3:\\n\", np.round(np.degrees(np.arccos(cos_sim))) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa84c7a-9108-4f95-85f8-c1ee0d8fc5df",
   "metadata": {},
   "source": [
    "### **Dot Product**\n",
    "The dot product is widely used to measure the similarity between two vectors. \n",
    "\n",
    "\\begin{equation*}\n",
    "\\vec{\\mathbf{a}} \\cdot \\vec{\\mathbf{b}} = \\sum_{i=1}^{d} a_i b_i\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\vec{\\mathbf{a}} \\cdot \\vec{\\mathbf{b}} = \\mathbf{a}^T \\mathbf{b}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\vec{\\mathbf{a}} \\cdot \\vec{\\mathbf{b}} = \\|\\vec{\\mathbf{a}}\\| \\times \\|\\vec{\\mathbf{b}}\\| \\times \\cos\\theta_{a,b}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "**Important Properties of Dot Product:**\n",
    "1. Two vectors are orthogonal (perpendicular) if their dot product is zero.\n",
    "\n",
    "2. The dot product of a vector with itself gives the square of its magnitude.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\vec{\\mathbf{a}} \\cdot \\vec{\\mathbf{a}} = \\|\\vec{\\mathbf{a}}\\|^2\n",
    "\\end{equation*}\n",
    "\n",
    "3. When the vectors are normalized (i.e., have unit length), the dot product is equivalent to the cosine of the angle between them.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\cos\\theta_{a,b} = \\frac{\\vec{\\mathbf{a}} \\cdot \\vec{\\mathbf{b}}}{\\|\\vec{\\mathbf{a}}\\| \\times \\|\\vec{\\mathbf{b}}\\|}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4dc526c-4655-4234-bd56-0da35d6ebefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot Product: 1.5\n",
      "Dot Product: [[1.5]]\n"
     ]
    }
   ],
   "source": [
    "# Way 1\n",
    "\n",
    "print(\"Dot Product:\", np.dot(vec_1, vec_2))\n",
    "\n",
    "# Computing magnitude of vec_1\n",
    "mag_vec_1 = np.linalg.norm(vec_1)\n",
    "\n",
    "# Computing magnitude of vec_2\n",
    "mag_vec_2 = np.linalg.norm(vec_2)\n",
    "\n",
    "# Computing cos theta between vec_1 and vec_2\n",
    "cos_sim = pairwise.cosine_similarity([vec_1], [vec_2])\n",
    "\n",
    "print(\"Dot Product:\", mag_vec_1 * mag_vec_2 * cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd89ed3d-4faa-48ec-a1bf-0d4c35e0b28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot Product: 1.5\n",
      "Dot Product: [[1.5]]\n"
     ]
    }
   ],
   "source": [
    "# Way 2\n",
    "\n",
    "print(\"Dot Product:\", np.dot(vec_1, vec_2))\n",
    "\n",
    "# Computing magnitude of vec_1\n",
    "mag_vec_1 = pairwise.euclidean_distances([[0, 0]], [vec_1])\n",
    "\n",
    "# Computing magnitude of vec_2\n",
    "mag_vec_2 = pairwise.euclidean_distances([[0, 0]], [vec_2])\n",
    "\n",
    "# Computing cos theta between vec_1 and vec_2\n",
    "cos_sim = pairwise.cosine_similarity([vec_1], [vec_2])\n",
    "\n",
    "print(\"Dot Product:\", mag_vec_1 * mag_vec_2 * cos_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d31ce81-55ed-4bc9-b495-1f9591c134c7",
   "metadata": {},
   "source": [
    "## **Matrix Algebra Operations**\n",
    "Matrix algebra operations play a crucial role in machine learning, as they form the foundation for many algorithms and techniques. Here are some key operations:\n",
    "1. Addition and Subtraction\n",
    "2. Scalar Multiplication\n",
    "3. Transpose\n",
    "4. Matrix Multiplication (Dot Product)\n",
    "5. Inverse\n",
    "6. Matrix Factorization\n",
    "7. Eigenvalues and Eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6ae3dc-a252-4349-9c9e-b8cdfb0a71bf",
   "metadata": {},
   "source": [
    "### **Matrix Multiplication (Dot Product)**\n",
    "Matrix multiplication, also known as the dot product, is an operation that takes two matrices and produces another matrix. It's a fundamental operation in linear algebra and plays a crucial role in various mathematical computations, including those in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fcde912-115d-42e2-b6fa-91489b20c438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Multiplication (A dot B):\n",
      " [[19 22]\n",
      " [43 50]]\n",
      "Matrix Multiplication (A dot B):\n",
      " [[19 22]\n",
      " [43 50]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example matrices\n",
    "A = np.array([[1, 2], [3, 4]])  # Shape: 2x2\n",
    "B = np.array([[5, 6], [7, 8]])  # Shape: 2x2\n",
    "\n",
    "# Way 1 -Matrix multiplication\n",
    "C = np.dot(A, B)\n",
    "print(\"Matrix Multiplication (A dot B):\\n\", C)\n",
    "\n",
    "# Way 2 - Matrix multiplication\n",
    "D = np.matmul(A, B)\n",
    "print(\"Matrix Multiplication (A dot B):\\n\", D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9cf59a-61dc-4f5f-b908-bb969b4b5a83",
   "metadata": {},
   "source": [
    "### **Matrix Inverse**\n",
    "\n",
    "The inverse of a matrix $ A $, denoted as $ A^{-1} $, is a matrix that, when multiplied by \\( A \\), results in the identity matrix $ I $. In other words, if $ A $ is an $ n \\times n $ matrix, then $ A^{-1} $ is also an $ n \\times n $ matrix such that:\n",
    "\n",
    "$$ A \\cdot A^{-1} = A^{-1} \\cdot A = I $$\n",
    "\n",
    "**Important Note**  \n",
    "However, not all matrices have inverses. In order for inverse to exist for a matrix, a matrix must be: \n",
    "- Square (i.e., have the same number of rows and columns)\n",
    "- Non-singular (i.e., its determinant must be non-zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0abd311a-e91e-48d9-bf49-793d51d20bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determinant: -2.0000000000000004\n",
      "Matrix A:\n",
      " [[1 2]\n",
      " [3 4]]\n",
      "Inverse of A:\n",
      " [[-2.   1. ]\n",
      " [ 1.5 -0.5]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example matrix\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "print(\"Determinant:\", np.linalg.det(A))\n",
    "\n",
    "# Compute the inverse of the matrix\n",
    "A_inv = np.linalg.inv(A)\n",
    "\n",
    "print(\"Matrix A:\\n\", A)\n",
    "print(\"Inverse of A:\\n\", A_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64206720-cd2a-4b4a-8b68-3167dd5aa0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determinant: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Example 2\n",
    "\n",
    "# Example matrix\n",
    "A = np.array([[3, 6], [2, 4]])\n",
    "\n",
    "print(\"Determinant:\", np.linalg.det(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11fbcb40-7bc0-4f9a-aede-a9da58a9e190",
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Compute the inverse of the matrix\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m A_inv \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMatrix A:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, A)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInverse of A:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, A_inv)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\linalg\\linalg.py:561\u001b[0m, in \u001b[0;36minv\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m    559\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->D\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->d\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    560\u001b[0m extobj \u001b[38;5;241m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001b[1;32m--> 561\u001b[0m ainv \u001b[38;5;241m=\u001b[39m \u001b[43m_umath_linalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap(ainv\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\linalg\\linalg.py:112\u001b[0m, in \u001b[0;36m_raise_linalgerror_singular\u001b[1;34m(err, flag)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_linalgerror_singular\u001b[39m(err, flag):\n\u001b[1;32m--> 112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSingular matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "# Compute the inverse of the matrix\n",
    "A_inv = np.linalg.inv(A)\n",
    "\n",
    "print(\"Matrix A:\\n\", A)\n",
    "print(\"Inverse of A:\\n\", A_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9687dabe-0214-4881-8aec-19123a125811",
   "metadata": {},
   "source": [
    "### **Matrix Factorization**\n",
    "\n",
    "Matrix factorization is a mathematical technique used to decompose a matrix into two or more matrices that, when multiplied together, approximate the original matrix. This decomposition is useful in various applications such as solving systems of linear equations, data compression, noise reduction, and more. \n",
    "\n",
    "Matrix factorization involves expressing a given matrix \\(A\\) as a product of two or more matrices. For example:\n",
    "\n",
    "$$ A = U \\cdot V $$\n",
    "\n",
    "Here, $ U $ and $ V $ are typically lower-dimensional matrices that, when multiplied together, approximate $ A $.\n",
    "\n",
    "\n",
    "Here are some common techniques:\n",
    "1. Singular Value Decomposition (SVD)\n",
    "2. Non-negative Matrix Factorization (NMF)\n",
    "\n",
    "\n",
    "#### **1. Singular Value Decomposition (SVD)**\n",
    "Given any `m x n` matrix $ A $, SVD decomposes it into three matrices: $ U $, $ \\Sigma $(a diagonal matrix of singular values), and $ V^T $. \n",
    "$$ A = U \\Sigma V^T  $$\n",
    "\n",
    "- $ A $ is the original `mxn` matrix.\n",
    "- $ U $ is an `mxm` orthogonal matrix (left singular vectors) which means $ U^TU=I_m $. These vectors represent an orthonormal basis for the column space of $ A $.\n",
    "- $ \\Sigma $ is an `mxn` diagonal matrix with non-negative real numbers on the diagonal (singular values). These are non-negative values and typically arranged in descending order.\n",
    "- $ V^T $ is an `nxn` orthogonal matrix (right singular vectors), meaning $ V^TV=I_n $. These vectors represent an orthonormal basis for the row space of $ A $.\n",
    "\n",
    "**Applications of SVD**  \n",
    "1. Dimensionality Reduction Techniques\n",
    "2. Recommendation Systems\n",
    "3. Topic Modelling\n",
    "\n",
    "**Checks Before Applying SVD**  \n",
    "1. **Matrix Dimensions**: SVD can be applied to any matrix, regardless of whether it is square.\n",
    "2. **Missing Values**: SVD cannot handle missing values. Ensure your matrix is complete or handle missing values appropriately.\n",
    "3. **Scaling**: Depending on the application, it might be useful to center or scale the data.\n",
    "\n",
    "**Step by Step Derivation**  \n",
    "Start with a Matrix $ A $. This matrix represents some data. For example, each row could be different items, and each column could be different features of those items.\n",
    "\n",
    "1. Compute $ A^TA $ and $ AA^T $:\n",
    "    - These steps help identify the directions (eigenvectors) and magnitudes (eigenvalues) along which the matrix varies the most.\n",
    "2. Form $ \\Sigma $:\n",
    "    - The singular values in $ \\Sigma $ tell us how much each direction (identified by $ U $ and $ V $) contributes to the structure of $ A $.\n",
    "3. Form $ U $ and $ V $:\n",
    "    - These matrices transform the original data into its principal components, helping us understand the underlying structure.\n",
    "4. Verify the decomposition\n",
    "    - Ensures our breakdown is accurate and can reconstruct the original matrix.\n",
    "\n",
    "\n",
    "#### **2. Non-negative Matrix Factorization (NMF)**\n",
    "NMF factors a matrix $ A $ into two matrices $ W $ and $ H $, where both matrices have no negative elements. \n",
    "$$ A \\approx WH $$\n",
    "**Checks Before Applying NMF**  \n",
    "1. **Non-negativity**: Ensure that the matrix $ A $ has no negative elements since NMF requires all elements to be non-negative.\n",
    "2. **Matrix Dimensions**: Like SVD, NMF can be applied to non-square matrices.\n",
    "3. **Initialization**: NMF can be sensitive to initialization. Different initializations might lead to different factorizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2db51fb9-b731-4b92-9671-c5b4508d4baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U:\n",
      " [[-1.99792151e-01  3.43948446e-02  8.90074954e-01 -4.08248290e-01]\n",
      " [-4.67812447e-01 -1.94307833e-01  2.77000566e-01  8.16496581e-01]\n",
      " [-7.35832744e-01 -4.23010511e-01 -3.36073823e-01 -4.08248290e-01]\n",
      " [-4.46972912e-01  8.84377503e-01 -1.34505196e-01  1.11022302e-16]]\n",
      "Sigma:\n",
      " [18.56522255  6.41002932  0.49399971]\n",
      "Vt:\n",
      " [[-0.38899998 -0.48867076 -0.78094808]\n",
      " [-0.57783044 -0.53080195  0.61996877]\n",
      " [-0.71748937  0.69242341 -0.07588689]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example matrix\n",
    "A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [0, 1, 10]])\n",
    "\n",
    "# Perform SVD\n",
    "U, sigma, Vt = np.linalg.svd(A)\n",
    "\n",
    "print(\"U:\\n\", U)\n",
    "print(\"Sigma:\\n\", sigma)\n",
    "print(\"Vt:\\n\", Vt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44d27faf-f9c9-4aec-bad7-7f616aa8745e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W:\n",
      " [[0.44706116 0.39494754]\n",
      " [0.76898491 1.22703153]\n",
      " [1.09090866 2.05911552]\n",
      " [1.81701696 0.        ]]\n",
      "H:\n",
      " [[0.         0.57593857 5.50084454]\n",
      " [3.34096864 3.63618467 1.45060291]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Example matrix\n",
    "A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [0, 1, 10]])\n",
    "\n",
    "# Perform NMF\n",
    "model = NMF(n_components=2, init='random', random_state=0)\n",
    "W = model.fit_transform(A)\n",
    "H = model.components_\n",
    "\n",
    "print(\"W:\\n\", W)\n",
    "print(\"H:\\n\", H)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbd06f1-041a-41fc-b5f7-a8ad3981f7bb",
   "metadata": {},
   "source": [
    "### **Eigenvalues and Eigenvectors**\n",
    "\n",
    "Eigenvalues and eigenvectors are concepts in linear algebra that are used to understand linear transformations.\n",
    "\n",
    "Given a square matrix $ A $, an eigenvector $ \\mathbf{v} $ and its corresponding eigenvalue $ \\lambda $ satisfy the equation:\n",
    "\n",
    "$$ A \\mathbf{v} = \\lambda \\mathbf{v} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33bdfd08-9360-4c62-b52d-02e96e74bc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues: [ 3. -1.]\n",
      "\n",
      "Eigenvectors:\n",
      "Eigenvalue: 3.0000000000000004\n",
      "Eigenvector: [0.70710678 0.70710678]\n",
      "Eigenvalue: -0.9999999999999996\n",
      "Eigenvector: [-0.70710678  0.70710678]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example matrix\n",
    "A = np.array([[1, 2], [2, 1]])\n",
    "\n",
    "# Compute eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "\n",
    "print(\"Eigenvalues:\", eigenvalues)\n",
    "print()\n",
    "print(\"Eigenvectors:\")\n",
    "for i in range(len(eigenvalues)):\n",
    "    print(\"Eigenvalue:\", eigenvalues[i])\n",
    "    print(\"Eigenvector:\", eigenvectors[:, i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
